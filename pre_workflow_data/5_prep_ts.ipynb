{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Address missing data, crop, detrend/normalize, and save timeseries netcdf\n",
    "\n",
    "Function of script:\n",
    "* Remove days with too few hours and years with too few days\n",
    "* Crop (e.g., ater 1960)\n",
    "* Update metadata record length\n",
    "* Detrend based on linear slope\n",
    "* Normalize (shift) to base period mean (e.g., 1995-2014) \n",
    "* Format and save as netcdf\n",
    "\n",
    "Context in workflow:\n",
    "* Must be run after preproc\n",
    "* stn_thresh and peaks-per-yr calculated separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "base_start = 1995 \n",
    "base_end = 2014\n",
    "missing_value = 999.999\n",
    "dec = 3\n",
    "max_missing = 0.3\n",
    "min_hrs_per_day = 12\n",
    "decluster = 25 # hours\n",
    "start_crop = '1960'\n",
    "\n",
    "DATA = Path(\"data\")\n",
    "INPUTS = DATA / \"inputs\"\n",
    "OUTPUTS = DATA / \"outputs\"\n",
    "\n",
    "stnlist_csv = INPUTS / \"metadata.csv\"\n",
    "stnlist_csv_edit = OUTPUTS / \"metadata.csv\"\n",
    "input_dir = OUTPUTS / \"wl_preproc\"\n",
    "output_dir = OUTPUTS / \"wl_proc\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "stnlist = pd.read_csv(stnlist_csv, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in stnlist.iterrows():\n",
    "    stn_num = str(row['stn_num']).zfill(5)\n",
    "    stn_name = row['stn_name']\n",
    "    datapath_in = os.path.join(input_dir, f\"{stn_num}_HOURLY.csv\") \n",
    "    wl_datapath_out = os.path.join(output_dir, f\"{stn_num}_wl.nc\") \n",
    "    wl_pot_datapath_out = os.path.join(output_dir, f\"{stn_num}_wl_pot.nc\") \n",
    "    print(stn_num, stn_name)\n",
    "    \n",
    "    df = pd.read_csv(datapath_in, encoding='latin-1', parse_dates=['DateTime_utc'], index_col='DateTime_utc')\n",
    "    df['wl_CGVD2013'] = df['wl_CGVD2013'].replace(missing_value, np.nan)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Remove days with too few hours \n",
    "    if not df.dropna().empty:\n",
    "        hrs_per_day = df.resample('D').count()\n",
    "        boolean_bad_days = hrs_per_day < min_hrs_per_day\n",
    "        bad_days = boolean_bad_days[boolean_bad_days].index\n",
    "        df = df[~df.index.isin(bad_days)]\n",
    "\n",
    "    # Remove years with too few days\n",
    "    if not df.dropna().empty:\n",
    "        days_per_year = df.dropna().resample('YE').apply(lambda x: pd.Index(x.index.date).nunique())\n",
    "        boolean_bad_yrs = days_per_year < (365.25*(1-max_missing))\n",
    "        bad_yrs = boolean_bad_yrs[boolean_bad_yrs['wl_CGVD2013']==True].index.year\n",
    "        df = df[~df.index.year.isin(bad_yrs)]\n",
    "        \n",
    "    # Crop after 1960\n",
    "    if not df.empty:\n",
    "        df = df.loc[df.index >= pd.to_datetime(start_crop).tz_localize('UTC')]\n",
    "        crop_nyrs_based_on_hrs = round(df['wl_CGVD2013'].dropna().count()/(365.25*24), 3)\n",
    "        crop_calendar_nyrs = len(df.dropna().index.year.unique())\n",
    "        crop_start_yr = int(df.index[0].year)\n",
    "        crop_end_yr = int(df.index[-1].year)\n",
    "        \n",
    "        if crop_nyrs_based_on_hrs > crop_calendar_nyrs:\n",
    "            raise ValueError(\"nyrs_based_on_hrs should be less than or equal to calendar_nyrs\")\n",
    "    else:\n",
    "        crop_nyrs_based_on_hrs, crop_calendar_nyrs, crop_start_yr, crop_end_yr = None, None, None, None\n",
    "    \n",
    "    # Update metadata after cropping\n",
    "    stnlist.at[i, 'calendar_nyrs'] = crop_calendar_nyrs\n",
    "    stnlist.at[i, 'nyrs_based_on_hrs'] = crop_nyrs_based_on_hrs\n",
    "    stnlist.at[i, 'start_yr'] = crop_start_yr\n",
    "    stnlist.at[i, 'end_yr'] = crop_end_yr\n",
    "    \n",
    "    # Detrend\n",
    "    df_yearly = df['wl_CGVD2013'].resample('YE').mean()\n",
    "    df_yearly = df_yearly.dropna()\n",
    "    time_ordinal_yearly = df_yearly.index.to_series().apply(lambda x: x.toordinal())\n",
    "    slope_per_day, intercept = np.polyfit(time_ordinal_yearly, df_yearly.values, 1)\n",
    "    time_ordinal_hourly = df.index.to_series().apply(lambda x: x.toordinal())\n",
    "    yearly_detrend_hourly_points = slope_per_day * time_ordinal_hourly + intercept\n",
    "    df[\"wl_CGVD2013_detrend\"] = df[\"wl_CGVD2013\"] - yearly_detrend_hourly_points\n",
    "    sl_m_per_yr = slope_per_day * 365.25 \n",
    "    sl_mm_per_yr = round(sl_m_per_yr * 1000, dec) \n",
    "    stnlist.at[i, 'sl_mm_per_yr'] = sl_mm_per_yr\n",
    "    \n",
    "    # Normalize\n",
    "    def mid_yr(base_start, base_end):\n",
    "        middle_year = (base_end - base_start) / 2 + base_start\n",
    "        year = int(middle_year)\n",
    "        fraction = middle_year - year\n",
    "        days = int(fraction * 365.25)\n",
    "        date = datetime(year, 1, 1) + pd.Timedelta(days=days)\n",
    "        return date\n",
    "    mid_yr_ordinal = mid_yr(base_start, base_end).toordinal()\n",
    "    residual_mean_after_detrend = df['wl_CGVD2013_detrend'].mean()\n",
    "    period_mean = slope_per_day*mid_yr_ordinal + intercept\n",
    "    df['wl_CGVD2013_detrend_norm'] = df['wl_CGVD2013_detrend'] - residual_mean_after_detrend + period_mean\n",
    "    \n",
    "    # Format & save netcdf file\n",
    "    df1 = df['wl_CGVD2013_detrend_norm'].reset_index()\n",
    "    df1.columns = ['time', 'wl']\n",
    "    wl = xr.DataArray(df1['wl'], dims=['time'], coords={'time': df1['time']}) #.rename(\"wl\")\n",
    "        \n",
    "    attrs = {\n",
    "        \"wl_stn_name\": stn_name,\n",
    "        \"wl_stn_id\": stn_num,\n",
    "        \"units\": \"m\",\n",
    "        \"standard_name\": \"sea_surface_height_above_geopotential_datum\",\n",
    "        \"geopotential_datum_name\": \"Canadian Geodetic Vertical Datum of 2013 (CGVD2013)\",\n",
    "        \"sl_mm_yr\": sl_mm_per_yr,\n",
    "        \"ref_period\": [base_start, base_end],\n",
    "        \"label\": \"Measured Water Level Timeseries\",\n",
    "        \"calendar_nyrs\": crop_calendar_nyrs,\n",
    "        \"intercept_ordinal\": intercept,\n",
    "    }\n",
    "    \n",
    "    with xr.set_options(keep_attrs=True):\n",
    "        wl.attrs.update(attrs)\n",
    "    wl.to_netcdf(wl_datapath_out)\n",
    "    \n",
    "stnlist.to_csv(stnlist_csv_edit, index=False, encoding='latin1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
